Você é um PAINEL com os 10 MAIORES ESPECIALISTAS EM QA DO MUNDO, cada um com foco diferente:
1) QA Líder de Estratégia (test plan + risco)
2) QA Backend/API (Java/Spring)
3) QA Frontend (React)
4) QA Banco de Dados (Postgres/Flyway)
5) QA Segurança (OWASP)
6) QA Performance (carga, stress)
7) QA Resiliência (falhas, timeouts, circuit breaker conceitual)
8) QA Observabilidade (logs/métricas/tracing)
9) QA Automação (CI/CD, pipelines)
10) QA Homologação/Negócio (regras duras e critérios de aceite)

CONTEXTO
Tenho uma aplicação chamada RULEX com:
- Backend: Java 21 + Spring Boot
- Frontend: React + Vite
- Banco: Postgres + Flyway migrations
- OpenAPI: openapi/rulex.yaml
- Motor de regras duras com simulação/homologação e logs/auditoria de execução
- Segurança de homologação: Basic Auth + RBAC (ADMIN e ANALYST)
RESTRIÇÃO: o payload de entrada não pode ser alterado.
OBJETIVO: Garantir que a solução está 100% funcional (sem gaps) para HOMOLOGAÇÃO.

SUA MISSÃO
Crie um PLANO DE TESTES COMPLETO e EXECUTÁVEL, com rigor máximo, incluindo:
(A) Matriz de testes por camada (UI, API, DB, regras, segurança)
(B) Casos de teste detalhados (Given/When/Then) com dados de exemplo
(C) Critérios de aceite objetivos (pass/fail) e evidências esperadas
(D) Priorização por risco (P0/P1/P2) e cobertura mínima necessária
(E) Estratégia de automação: quais testes automatizar, quais manuais e por quê
(F) Checklist final de homologação “go/no-go”

FORMATO DE SAÍDA OBRIGATÓRIO
1) Visão geral (escopo, fora de escopo, riscos)
2) Matriz (tabela) → Tipo de teste | Objetivo | Ferramenta | Ambiente | Prioridade
3) Suite P0 (obrigatória) com no mínimo:
   - 20 testes de API (incluindo erros, validação, status code, contratos)
   - 10 testes de UI (fluxos críticos com React Testing Library/Cypress)
   - 10 testes de regras duras (cenários reais e edge cases)
   - 8 testes de DB (migrations, constraints, índices, integridade)
   - 10 testes de segurança (OWASP + RBAC + vazamento de dados)
   - 6 testes de performance (latência, throughput, picos)
   - 6 testes de resiliência (DB down, timeout, 5xx, retry/idempotência)
   - 6 testes de observabilidade (logs corretos, correlation id, auditoria)
4) Suite P1 (robustez) e P2 (nice-to-have)
5) Plano de dados de teste (massa de dados) + estratégia de limpeza/reset
6) Definição de ambientes (dev/test/homolog) e variáveis necessárias
7) Saída com exemplos práticos:
   - Exemplos de requests cURL para endpoints (incluindo auth)
   - Exemplos de payloads válidos e inválidos (SEM alterar o payload real)
   - Exemplos de asserts esperados
   - Exemplo de relatório de execução e como armazenar evidências

EXIGÊNCIAS TÉCNICAS (NÃO IGNORAR)
- Validar conformidade com OpenAPI (contrato)
- Verificar CORS (frontend -> backend) e erros comuns de rede
- Verificar concorrência: múltiplas execuções simultâneas do motor de regras
- Verificar idempotência / duplicidade quando o mesmo payload chega repetido
- Verificar persistência de logs/audit (não pode perder trilha)
- Verificar mascaramento de dados sensíveis (PAN, etc.) em logs e responses
- Verificar RBAC por endpoint (ADMIN vs ANALYST) + tentativa de bypass
- Verificar SQL Injection, XSS em campos exibidos na UI e em logs
- Verificar migrations Flyway: fresh install, upgrade, rollback strategy (se aplicável)
- Verificar respostas de erro padronizadas (4xx/5xx) sem stacktrace em produção
- Verificar observabilidade: logs por nível, ausência de DEBUG em homolog/prod

REGRAS DE RIGOR
- Para cada teste, inclua: ID, Objetivo, Pré-condição, Passos, Dados, Resultado esperado, Severidade, Automação (sim/não)
- Crie também uma seção “TESTES QUE NORMALMENTE AS PESSOAS ESQUECEM” (mínimo 15)
- Se você identificar lacunas na solução (falta de logs, falta de endpoint de health, etc.), liste como “GAPS” e proponha o teste + a correção sugerida.

IMPORTANTE
Não seja genérico. Seja específico, detalhado e prático. Se precisar assumir nomes de endpoints, baseie-se no OpenAPI e na estrutura típica (ex.: /api/...).
Entregue a resposta final como se fosse um documento pronto para homologação.


Você é um PAINEL com os 10 MAIORES ESPECIALISTAS EM QA DO MUNDO, com autoridade para reprovar a homologação. 
Você fará um DOUBLE CHECK 100x mais rigoroso do que um QA normal.

CONTEXTO
Aplicação RULEX:
- Backend: Java 21 + Spring Boot
- Frontend: React + Vite
- Banco: Postgres + Flyway
- OpenAPI: openapi/rulex.yaml
- Motor de regras duras + simulação/homologação + auditoria/logs de execução
- Segurança: Basic Auth + RBAC (ADMIN e ANALYST)
RESTRIÇÃO CRÍTICA: o payload de entrada NÃO PODE SER ALTERADO.
OBJETIVO: garantir a solução 100% funcional e segura para HOMOLOGAÇÃO, sem gaps.

SUA MISSÃO
Você deve produzir um PLANO DE TESTES EXECUTÁVEL + MATRIZ + CASOS DETALHADOS + DADOS DE TESTE + CRITÉRIOS GO/NO-GO.
Não seja genérico. Não escreva “testar login” — escreva casos completos (Given/When/Then) com inputs e asserts.

REGRAS DE SAÍDA (OBRIGATÓRIO)
1) Test Strategy (escopo, fora de escopo, riscos, premissas, ambientes)
2) Matriz de testes (tabela) por camada e tipo
3) Test Suites: P0, P1, P2
4) Para CADA teste: ID, Título, Objetivo, Risco, Prioridade, Pré-condição, Passos, Dados, Expected, Evidência, Automação (Sim/Não), Dono (API/UI/DB/Sec)
5) Rastreabilidade: cada requisito/endpoint/regra precisa apontar para testes (trace matrix)
6) Evidências: o que salvar (logs, screenshots, JSON de response, auditoria no DB, relatórios de coverage)
7) Checklist final de homologação: GO/NO-GO com critérios objetivos.

MÍNIMOS ABSOLUTOS (P0) — SE FALTAR, REPROVA
Você deve listar NO MÍNIMO:

A) CONTRATO & API (mín. 40 testes)
- Validar OpenAPI (request/response) endpoint por endpoint
- Status codes corretos (200/201/204/400/401/403/404/409/422/429/500)
- Schema validation: tipos, obrigatórios, formatos, limites
- Backward compatibility (versionamento se existir)
- Idempotência em endpoints que processam payloads repetidos
- Erros padronizados (não vazar stacktrace)
- CORS + preflight (OPTIONS) + headers corretos
- Rate limiting (mesmo que “não implementado”, deve virar GAP)
- Pagination/filter/sort (se existir) com testes de borda
- Headers de segurança e cache (se aplicável)

B) MOTOR DE REGRAS (mín. 35 testes)
- Cenários positivos/negativos por regra dura (inclua edge cases)
- Prioridade e conflito entre regras (duas regras ativam, qual vence?)
- Determinismo: mesma entrada -> mesmo resultado sempre
- Regras com valores nulos/ausentes (sem alterar payload)
- Limites numéricos (min/max), datas inválidas, timezone, moeda, arredondamento
- Strings com espaços, unicode, caracteres especiais, case sensitivity
- Reprocessamento do mesmo payload (duplicidade)
- Concorrência: 50+ execuções simultâneas do motor (thread-safety)
- Logs/auditoria por execução (deve persistir e ser consultável)
- Mascaramento de dados sensíveis nos outputs e logs

C) BANCO DE DADOS & MIGRATIONS (mín. 20 testes)
- Fresh install: DB vazio -> sobe -> Flyway aplica tudo
- Re-execução: migrations idempotentes (Flyway não quebra)
- Constraints: NOT NULL, FK, unique (onde aplicável)
- Integridade e consistência do audit log
- Índices: existência e impacto (explain basic nas queries críticas)
- Transações: rollback em erro (sem “meia gravação”)
- Deadlocks e concorrência (simular)
- Data retention (se existe), limpeza e governança (se não existe, GAP)
- Timezone/locale no DB

D) SEGURANÇA (mín. 25 testes)
- RBAC por endpoint: ADMIN vs ANALYST (401 vs 403 corretamente)
- Tentativas de bypass (paths alternativos, query params, methods)
- Injeção: SQLi, XSS (refletido/armazenado), command injection (se houver)
- Sensitive data exposure: PAN/PII em logs, responses e UI
- CORS policy (origens permitidas)
- Bruteforce/basic auth (mitigação ou GAP)
- Session/cookies (se usados)
- Security headers: CSP, HSTS, X-Content-Type-Options, etc. (se não houver, GAP)
- Dependências vulneráveis (SCA): listar libs e recomendar scan (OWASP Dependency-Check/NPM audit)

E) FRONTEND UI/UX (mín. 25 testes)
- Fluxos críticos ponta a ponta: listar e testar
- Estados de loading/erro/vazio
- Validação de formulários (sem quebrar o payload)
- Consistência de mensagens e ações de retry
- Acessibilidade (mínimo): foco, navegação por teclado, labels, contraste (se não fizer, GAP)
- Cross-browser básico (Chrome/Edge/Firefox)
- Responsividade (desktop/tablet)
- XSS via campos renderizados
- Cache e stale data (troca de tela, voltar, refresh)
- Falhas de rede: timeout, 500, 401/403 — UI reage corretamente (redirect, aviso, retry)

F) OBSERVABILIDADE & OPERAÇÃO (mín. 15 testes)
- Logs: nível correto (INFO em homolog), sem DEBUG (ou GAP)
- Correlation ID por request e propagação
- Audit trail: cada execução do motor gera registro consultável
- Health/readiness endpoint (se não existir, GAP)
- Métricas mínimas (latência, erro) (se não existir, GAP)
- Alertabilidade: quais sinais seriam monitorados (documentar)

G) PERFORMANCE (mín. 12 testes)
- Latência P50/P95/P99 em endpoints críticos
- Throughput com carga progressiva
- Spike test (pico)
- Soak test curto (30–60 min)
- Teste de payload grande (sem alterar schema)
- CPU/memória (sem vazamento)
- DB under load: conexões, pool saturado

H) RESILIÊNCIA/FAULT INJECTION (mín. 12 testes)
- DB down durante processamento
- Timeout de DB
- Erros 5xx internos
- Rede instável para UI
- Requisições duplicadas (replay)
- Retentativa manual do usuário sem duplicar efeitos
- Recovery após falha (subiu de novo, sistema consistente)

I) DEVEX/CI (mín. 8 itens)
- Comandos limpos: mvn test, mvn verify, pnpm test, build
- Pipeline sugerido e gates: lint, tests, coverage, SCA, container build
- Versionamento de artifacts
- Ambiente reproducível com docker compose

EXIGÊNCIA EXTRA: "TESTES QUE NORMALMENTE AS PESSOAS ESQUECEM" (mín. 30)
Liste e inclua casos para:
- Preflight OPTIONS (CORS)
- Timezone e datas em virada de dia/hora (DST se aplicável)
- Números com vírgula/ponto
- Strings com unicode/emoji
- Campos muito longos (limites)
- Campos vazios vs ausentes
- Null vs "" vs 0
- Ordenação determinística de listas
- Erro 401 vs 403 (diferença correta)
- Refresh do navegador no meio do fluxo
- Duplo clique / submit repetido
- Retry após 500
- Falha parcial no meio da gravação
- Pool de conexão saturado
- Logs com dados sensíveis
- Inconsistência de auditoria
- Duplicidade de registro
- Regras conflitantes e precedência
- Concorrência: race condition em execução de regra
- Migração rodando em DB já com dados
- Falha de build em ambiente limpo
- Config por env faltando (prod) -> falha segura
- Headers ausentes
- Cache indevido no frontend
- Resposta lenta (UI não trava)
- Limpeza de massa de teste
- Rollback manual (se não existe, GAP)
- Dependências vulneráveis (npm/maven)
- Falha de parsing de JSON
- Campos extras no payload (robustez)
- Campos fora de ordem (robustez)
- Encoding UTF-8
- “Clock skew” (hora do servidor diferente)

GAPS
Se você identificar qualquer funcionalidade ausente para homologação (ex.: falta health, falta rate limit, falta métricas, falta hardening de security headers, falta teste E2E, etc.), escreva em seção “GAPS CRÍTICOS” com:
- Impacto
- Como testar
- Correção recomendada (mínima para homologação)

IMPORTANTE FINAL
- Não aceite “ficou bom”.
- Entregue tudo como documento pronto para homologação.
- Se faltar informação (nome de endpoints), infira pelo OpenAPI e pela estrutura, mas deixe explícito o que é inferência.



✅ TRIPLO CHECK 1 — P0 “Reprova se faltar” (Homologação)
API/Contrato (obrigatório)

 Validação endpoint por endpoint contra OpenAPI (request e response)

 Testes de status: 200/201/204/400/401/403/404/409/422/500

 Erro padronizado (sem stacktrace em response)

 CORS + preflight OPTIONS

 Headers corretos (content-type, cache, security quando aplicável)

 Idempotência / replay do mesmo payload (sem duplicar efeitos)

 Rate limit / proteção contra abuso (se não existe → GAP crítico)

Motor de Regras Duras (obrigatório)

 1 teste por regra (positivo e negativo)

 Conflito de regras (duas ativam: precedência determinística)

 Campos ausentes/null/""/0 (sem alterar payload)

 Datas/timezone/moeda/arredondamento

 Strings longas, unicode, caracteres especiais

 Determinismo (mesma entrada → mesmo resultado)

 Concorrência (ex.: 50+ execuções simultâneas)

 Auditoria persistida e consultável por execução

Banco (obrigatório)

 Fresh install Flyway (DB vazio)

 Reexecução sem quebrar

 Constraints (NOT NULL/FK/Unique onde aplicável)

 Transação/rollback em erro (sem meia gravação)

 Índices em queries críticas (validar com explain básico)

 Auditoria consistente (não perde trilha)

Segurança (obrigatório)

 RBAC por endpoint (ADMIN vs ANALYST) com 401 vs 403 correto

 Tentativa de bypass (método HTTP diferente, path alternativo)

 Sensitive data exposure: PAN/PII em logs/response/UI

 SQL Injection / XSS (refletido e armazenado)

 Vulnerabilidade em dependências (npm audit / maven scan)

 Senhas/segredos fora do código (env/profile)

Frontend (obrigatório)

 Fluxos críticos ponta a ponta (do UI até API e DB)

 Loading/erro/vazio (UX mínimo)

 401/403 (UI reage corretamente)

 Falha de rede/timeout (retry/feedback)

 Validação de formulário sem alterar payload

 XSS em campos exibidos

Observabilidade/Operação (obrigatório)

 Logs com nível correto (INFO em homolog)

 Correlation ID por request (ou GAP)

 Health/readiness endpoint (ou GAP)

 Auditoria e trilha verificável (evidência)

 Evidências salvas (logs, screenshots, JSONs)

Performance/Resiliência (obrigatório)

 k6/JMeter: P50/P95/P99 em endpoints críticos

 Spike test (pico)

 DB down durante execução (falha controlada)

 Replay do mesmo payload sob carga

 Soak curto (30–60 min) para detectar leak

Se qualquer bloco acima não existir, o plano não é “100%”.

✅ TRIPLO CHECK 2 — Matriz de cobertura (pra não sobrar “área sem dono”)

Confere se você tem testes cobrindo cada dimensão:

Camadas

UI (React)

API (controllers + validação)

Domínio (regras)

Persistência (repos/DB)

Infra (config/env/profile)

Tipos de teste

Funcional (happy path)

Negativo (erros/validação)

Segurança (OWASP + RBAC)

Contrato (OpenAPI)

Integração (backend+db)

E2E (UI→API→DB)

Performance (carga, spike, soak)

Resiliência (falhas)

Observabilidade (logs/audit)

Regressão (suite curta P0)

Rastreabilidade (essencial)

Cada endpoint → pelo menos 2 testes (positivo/negativo)

Cada regra dura → pelo menos 2 testes (ativa/não ativa)

Cada role (ADMIN/ANALYST) → matriz de acesso por endpoint

Cada migration → test de fresh install e upgrade

✅ TRIPLO CHECK 3 — “Buracos clássicos” que quase sempre faltam

Se o seu plano não tiver isso, ainda tem gap:

Preflight OPTIONS (CORS)

Diferença correta de 401 vs 403

Duplo submit / double click / retry manual

Campos: null vs ausente vs "" vs 0

Unicode/emoji/acentos/encoding UTF-8

Datas em virada de dia/hora/timezone

Payload com campos extras (robustez)

Ordem de campos diferente (robustez)

Auditoria perde trilha em falhas parciais

Logs vazando dados (PAN/PII)

Conexões DB saturadas (pool)

Concorrência/race condition no motor

Fresh install vs DB já com dados

Build em ambiente limpo (sem “dep escondida”)

Config faltando em homolog (falha segura)


Você é um auditor QA “TRIPLO CHECK” (3 passadas) e vai revisar o plano de testes pro sistema RULEX.
Objetivo: provar se ainda existem gaps e listar exatamente quais testes faltam.

Entrada: (cole aqui o plano de testes atual)

Passada 1 (Cobertura): verifique se todas as dimensões estão cobertas:
UI, API, contrato OpenAPI, regras duras, DB/Flyway, segurança (RBAC/OWASP), observabilidade, performance, resiliência.

Passada 2 (Risco): crie uma matriz risco×impacto e marque o que é P0 (reprova), P1, P2.
Se algo P0 estiver ausente, liste como GAP CRÍTICO.

Passada 3 (Esquecidos): compare o plano com uma lista de “30 testes que as pessoas esquecem”
(preflight OPTIONS, 401 vs 403, duplo submit, null vs ausente vs "", unicode, timezone, replay/idempotência, concorrência, vazamento em logs, pool saturado etc.)
e diga quais não estão presentes.

Saída obrigatória:
1) Lista de GAPS CRÍTICOS (com o teste exato que falta)
2) Lista de GAPS IMPORTANTES
3) Lista de melhorias (nice-to-have)
4) Nova Suite P0 final com IDs e Given/When/Then para cada teste faltante.
Não seja genérico. Escreva casos executáveis.
